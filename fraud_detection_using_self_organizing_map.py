# -*- coding: utf-8 -*-
"""fraud_detection_using_self_organizing_map.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hun7KztH3w7phg4nFofjTiVlYbCqh6FJ

Dataset: https://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval)

This file concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data.

This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.

There are 6 numerical and 8 categorical attributes. The labels have been changed for the convenience of the statistical algorithms. For example, attribute 4 originally had 3 labels p,g,gg and these have been changed to labels 1,2,3.

A1: 0,1 CATEGORICAL (formerly: a,b)

A2: continuous.

A3: continuous.

A4: 1,2,3 CATEGORICAL (formerly: p,g,gg)

A5: 1, 2,3,4,5, 6,7,8,9,10,11,12,13,14 CATEGORICAL (formerly: ff,d,i,k,j,aa,m,c,w, e, q, r,cc, x)

A6: 1, 2,3, 4,5,6,7,8,9 CATEGORICAL (formerly: ff,dd,j,bb,v,n,o,h,z)

A7: continuous.

A8: 1, 0 CATEGORICAL (formerly: t, f)

A9: 1, 0 CATEGORICAL (formerly: t, f)

A10: continuous.

A11: 1, 0 CATEGORICAL (formerly t, f)

A12: 1, 2, 3 CATEGORICAL (formerly: s, g, p)

A13: continuous.

A14: continuous.

A15/class: 1,2 class attribute (formerly: +,-) or (1: Approved or 0: Not Approved)
"""

!pip install MiniSom

"""Importing MiniSom library"""

from minisom import MiniSom

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

dataset=pd.read_csv("Credit_Card_Applications.csv")
dataset.describe()

X=dataset.iloc[:,0:-1].values
Y=dataset.iloc[:,-1].values

"""Data Normalization"""

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
X_scaled=scaler.fit_transform(X)

print(X)

print(X_scaled)

"""SOM initializaton and training"""

n_neuron=10
m_neuron=10
# input_len=number of features or, number of columuns(excluding target)
som=MiniSom(n_neuron, m_neuron, input_len=X_scaled.shape[1], sigma=1, 
            learning_rate=0.5, neighborhood_function='gaussian')
som.random_weights_init(X_scaled)
som.train(X_scaled, 100)

som.distance_map()
#win_map diye kono ekta winning node kon kon sample er jonne hoiche seta bujhaay
mappings=som.win_map(X_scaled,return_indices=True)
#mappings holo ekta dictionary type er container
count=0
for i in mappings:
    print("for coordinate no. ", i)
    print("It is the winning node for sample no. :")
    for j in mappings.get(i):
        print(j)
        count=count+1
print(mappings)
print(count)

"""Visualization

"""

# Commented out IPython magic to ensure Python compatibility.
#originally:
#red= 0 means NOT Approved by the bank
#green= 1 means Approved by the bank

#in color graph:
#white=fraud cluster
#black=good cluster
# %matplotlib inline
plt.figure(figsize=(10,10))
plt.pcolor(som.distance_map().T, cmap='bone')
plt.colorbar()

markers=['o','*']
colors=['r','g']
for i, xx in enumerate(X_scaled):
    w=som.winner(xx)
    plt.plot(w[0]+0.5,w[1]+0.5,markers[Y[i]],markerfacecolor='None',
             markeredgecolor=colors[Y[i]],markersize=15,markeredgewidth=2)
plt.show()

#decision:
#black+red--> Bank did NOT approve a good people
#black+green-->Bank approved a good people
#white+green--> Bank approved a FRAUD people
#white+red--> Bank did not approve a FRAUD people

"""Detecting obvious frauds"""

#at that time coordinates (1,6),(1,7),(1,8) were more white-ish, so they are the obvious frauds
frauds=np.concatenate((mappings.get((1,6)),mappings.get((1,7)),mappings.get((1,8))),axis=0)
for i in frauds:
    print(int(X[i,0]))